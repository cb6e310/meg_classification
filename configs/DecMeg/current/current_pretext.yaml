EXPERIMENT:
  NAME: "current_pretext"
  TAG: "proposed"
  PROJECT: "proposed project"
  DEBUG: False 
  LOG_IMAGES: False
  REPETITION_NUM: 3 # Number of repetition times
  TASK: "pretext"
  RESUME : False # Resume training
  CHECKPOINT_GAP: 10
  SEED: 42
  KNN: True
  EVAL_ONLY: False
  EVAL_NEXT: True
  EVAL_LINEAR: True
  PRETRAINED_PATH: "/home/song/code/current/meg_classification/ssl/results/baseline project/InfoTS_Pretrain_2024-07-04_11-20-21"
DATASET:
  ROOT: "/home/song/datasets/MEG"
  TYPE: "DecMeg"
  CHANNELS: 204
  POINTS: 250
  NUM_CLASSES: 2
  TEST:
    BATCH_SIZE: 128
MODEL:
  TYPE: "CurrentCLR"
  ARGS:
    BACKBONE: "varcnn"
    SIAMESE: True
    USE_MOMENTUM: True  # True for BYOL, False for SimSiam
    HIDDEN_LAYER: -1
    PROJECTION_DIM: 512
    PROJECTION_HIDDEN_SIZE: 4096
    TAU_BASE: 0.996
    N_FEATURES: 45000
    WARMUP_EPOCHS: -1
    REC_WEIGHT: 1
    CLS_WEIGHT: 0.1
    PRED_WEIGHT: 0.1
    SOURCE_CHANNELS: 360
  CRITERION:
    TYPE: "RegressionLoss"
      # SOURCE_CHANNELS: 36
SOLVER:
  TRAINER: "current"
  BATCH_SIZE: 512
  EPOCHS: 10
  LR: 0.05
  LAMBDA_L1: 0.000001
  SCHEDULER:
    TYPE: "ExponentialLR"
    GAMMA: 0.99
    MIN_LR: 0.0001
EVAL_LINEAR:
  CLASSIFIER: "LinearClassifier"
  BATCH_SIZE: 512
  EPOCHS: 200
  LR: 0.5
  CRITERION: "CE"
  SCHEDULER:
    TYPE: "ExponentialLR"
    GAMMA: 0.99
    MIN_LR: 0.001
